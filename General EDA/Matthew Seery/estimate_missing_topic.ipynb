{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edad6b0-81ca-458f-871a-f7488027e29d",
   "metadata": {},
   "source": [
    "# PREDICT VALUE FOR MISSING QUESTION TOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5ef8-99bd-43a8-83a0-b4c184034fb0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c538316c-9abf-479f-8e67-0b56afd4637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 20:27:40.329482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764322060.395360    2232 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764322060.415665    2232 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764322060.701723    2232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764322060.701808    2232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764322060.701813    2232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764322060.701816    2232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_MIN_GPU_MULTIPROCESSOR_COUNT'] = '6' # Needed so I can use my old GPU with the new one\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # Turns off oneDNN custom operations\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # Hides message regarding TensorFlow optimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2991ae-3112-4dde-8da6-0d5b7daa2d32",
   "metadata": {},
   "source": [
    "## Import CSV File & Remove Duplicates\n",
    "NB:\n",
    "\n",
    "'question_topic_valid.csv' represents records from the original dataset where 'question_topic' was not empty while 'question_topic_null.csv' are records with missing values for 'question_topic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbb4951-125c-42ab-9767-4b03b73c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import patitioned libraries of original dataset\n",
    "# These datasets were created in another notebook (see 'dataset_partition.ipynb')\n",
    "df_topic_exists = pd.read_csv('../data/question_topic_valid.csv', usecols=[0,2,3,4,13,14]) # Import only essential columns\n",
    "df_topic_null = pd.read_csv('../data/question_topic_null.csv')\n",
    "\n",
    "# Drops duplicate 'question_content' due to multiple 'question_id' in the dataset\n",
    "df_topic_exists.drop_duplicates(subset='question_id',inplace=True)\n",
    "df_topic_exists.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafbd64-c133-4cf5-b57a-e331a75582b7",
   "metadata": {},
   "source": [
    "## Tokenize The Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243fcdda-d3c0-4493-bfe0-09c99ac6a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_sentences(col):\n",
    "    sentence_list = []\n",
    "    for text in col:\n",
    "        splitted_text = text.lower().split()\n",
    "        sentence_list.append(splitted_text)\n",
    "    return sentence_list\n",
    "\n",
    "sentences_topic_exist = list_of_sentences(df_topic_exists.question_content)\n",
    "sentences_topic_null = list_of_sentences(df_topic_null.question_content)\n",
    "\n",
    "# Initiate the tokenizer with a out_of_vocabulary token \n",
    "tokenizer_X = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# Generate word indexes for all sentences \n",
    "tokenizer_X.fit_on_texts(sentences_topic_exist+sentences_topic_null)\n",
    "\n",
    "# Generate separate sequences for both with topic values and missing values\n",
    "X = tokenizer_X.texts_to_sequences(sentences_topic_exist)\n",
    "X_topic_null = tokenizer_X.texts_to_sequences(sentences_topic_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6956b-9f6c-4a3d-8767-807e1aeb5ce1",
   "metadata": {},
   "source": [
    "## Determine Word Counts & Maximum Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f803052c-b3ca-4469-92e3-bd784941bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words from all questions is 1292953.\n",
      "The highest number of words in any sentence is 197.\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of words from all questions is {len(tokenizer_X.word_counts)}.')\n",
    "\n",
    "max_len = 0\n",
    "for l in X + X_topic_null: # Include questions from the entire dataset\n",
    "    if len(l) > max_len:\n",
    "        max_len = len(l)\n",
    "\n",
    "print(f'The highest number of words in any sentence is {max_len}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4942d2bf-9b4c-4dd7-9e4f-240485a8d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 40000     # Use 40000 most frequent words from the total of 1292953 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5993d23-c057-4b78-b556-5cd8d536268b",
   "metadata": {},
   "source": [
    "## Create Train & Test Datasets & Prepare For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e1535-0d21-40bb-8425-885f79bc7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, df_topic_exists.question_topic, test_size=0.2,\n",
    "                                                                stratify=df_topic_exists.question_topic, random_state=42)\n",
    "\n",
    "# Format X and y for model\n",
    "X_train = np.array(sequence.pad_sequences(X_train_df, maxlen=max_len))\n",
    "X_test = np.array(sequence.pad_sequences(X_test_df, maxlen=max_len))\n",
    "\n",
    "y_train_one_hot = pd.get_dummies(y_train_df)\n",
    "y_train = y_train_one_hot.to_numpy()\n",
    "y_test_one_hot = pd.get_dummies(y_test_df)\n",
    "y_test = y_test_one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb8495-742f-4d35-ab60-7ffd05c21821",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f680d6e-d724-4ba7-b20d-fdd862fbf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block class\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e8d561-5861-4b89-86e1-c73ef6064639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764322795.785312    2232 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1764322795.790802    2232 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2857 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Define the model with an embedding layer, transformer block, and output layer\n",
    "embed_dim = 32 # Embedding dimension for each word vector\n",
    "num_heads = 4  # The number of attention heads in the multi-head attention layer\n",
    "ff_dim = 64    # Number of units in the feed forward layer\n",
    "\n",
    "inputs = layers.Input(shape=(max_len,))\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=max_features, output_dim=embed_dim)\n",
    "out = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "out = transformer_block(out, training=True)\n",
    "out = layers.GlobalAveragePooling1D()(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "out = layers.Dense(20, activation='relu')(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "outputs = layers.Dense(148, activation='softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569ba3-a84f-4fe5-b6cc-0d6145c09d6a",
   "metadata": {},
   "source": [
    "## Compile & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945f82e3-e324-47d5-a88e-835800da38c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 20:40:00.650046: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2115021944 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764322813.155020    2485 service.cc:152] XLA service 0x7af2e0009600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764322813.155119    2485 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "I0000 00:00:1764322813.155125    2485 service.cc:160]   StreamExecutor device (1): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "I0000 00:00:1764322813.977780    2485 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1764322833.795042    2485 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 43ms/step - accuracy: 0.8413 - loss: 0.6320 - val_accuracy: 0.8999 - val_loss: 0.3170\n",
      "Epoch 2/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 38ms/step - accuracy: 0.8940 - loss: 0.3441 - val_accuracy: 0.9019 - val_loss: 0.2866\n",
      "Epoch 3/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 38ms/step - accuracy: 0.8974 - loss: 0.3146 - val_accuracy: 0.9017 - val_loss: 0.2798\n",
      "Epoch 4/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 38ms/step - accuracy: 0.8993 - loss: 0.2964 - val_accuracy: 0.9026 - val_loss: 0.2722\n",
      "Epoch 5/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 38ms/step - accuracy: 0.9013 - loss: 0.2833 - val_accuracy: 0.9030 - val_loss: 0.2723\n",
      "Epoch 6/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9029 - loss: 0.2733 - val_accuracy: 0.9027 - val_loss: 0.2716\n",
      "Epoch 7/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9043 - loss: 0.2653 - val_accuracy: 0.9024 - val_loss: 0.2748\n",
      "Epoch 8/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 38ms/step - accuracy: 0.9054 - loss: 0.2587 - val_accuracy: 0.9022 - val_loss: 0.2733\n",
      "Epoch 9/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 38ms/step - accuracy: 0.9066 - loss: 0.2526 - val_accuracy: 0.9025 - val_loss: 0.2819\n",
      "Epoch 10/10\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 38ms/step - accuracy: 0.9076 - loss: 0.2477 - val_accuracy: 0.9029 - val_loss: 0.2887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7af4bd0a9850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dd374-2685-4c79-86ea-3e9bdc6407f5",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54c3f34-1aeb-4524-bcb5-92f63cd4f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26212/26212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 5ms/step - accuracy: 0.9027 - loss: 0.2879\n",
      "Test Accuracy: 0.9027340412139893\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83c435-514d-414d-8502-f4137e57f446",
   "metadata": {},
   "source": [
    "## Extract Failed Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807fcf2b-18d8-4695-9d1d-ce6a273290a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26212/26212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Store in a list the column names for one-hot encoding (question_topic)\n",
    "one_hot_columns = list(y_test_one_hot.columns)\n",
    "\n",
    "# Store predictions for X_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Add predictions column to y_test_df\n",
    "y_test_df = y_test_df.to_frame()\n",
    "y_test_df['predictions'] = [one_hot_columns[i] for i in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "# Merge index associated rows from the original source dataset along with the predictions \n",
    "test_df = pd.merge(df_topic_exists, y_test_df, left_index=True, right_index=True)\n",
    "\n",
    "# Create new dataframe that stores rows from test df where predictions were incorrect plus adds the predictions column\n",
    "false_predictions = pd.DataFrame()\n",
    "for i,v in test_df.iterrows():\n",
    "    if v.question_topic_x != v.predictions:\n",
    "        row = pd.DataFrame({'question_language' : [v.question_language], 'question_content' : [v.question_content],\n",
    "                            'question_user_status' : [v.question_user_status], 'question_user_country_code' : [v.question_user_country_code],\n",
    "                            'question_topic': [v.question_topic_x], 'predictions' : [v.predictions]\n",
    "                            })\n",
    "        false_predictions = pd.concat([false_predictions, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01507607-286b-4f4b-94a4-b8d4add2b628",
   "metadata": {},
   "source": [
    "# Export Test df For Predictions Versus Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02586d05-fa9f-4a02-8847-fb6e5faaf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/prediction_vs_actual_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace49208-9f2c-4afe-b1f7-78da0638fc4a",
   "metadata": {},
   "source": [
    "## Check For Any Indicators For Failure Rate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92605c71-b205-4886-9f8c-8a5b2ce50567",
   "metadata": {},
   "source": [
    "The overall failure rate for the test data set is 9.73%.\n",
    "\n",
    "For question_language, question_user_country_code, and question_user_status, the percentage failure rates are fairly consistent with the overall failure rate.\n",
    "\n",
    "For question_topic, topics with the lowest counts tended to have very high failure rates presumably because there were not enough samples to learn from. The one noticeable exception to this was 'plant' which had a failure rate of 42.55% despite there being 27895 samples in the test dataset. Possibly 'plant' is too generic given that there are other plants listed in the topics.\n",
    "\n",
    "The topics in question_topic with lower failure rates tended to have higher counts although there were some values that performed well despite the very small sample size. The topic 'rabbit' performed best with only 2.62% failure rate. There were 17 topics with a sample size less than 30 that could not make even 1 correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cac0a96-50a4-45de-a6f6-5ec092e79ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test data % failure rate by question_language\n",
      "eng     9.870407\n",
      "swa    10.101619\n",
      "nyn     6.234610\n",
      "lug     9.307695\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_country_code\n",
      "ke    11.804131\n",
      "ug     8.623341\n",
      "tz     7.363588\n",
      "gb     8.333333\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_status\n",
      "live          9.371735\n",
      "zombie       10.544332\n",
      "destroyed    10.615150\n",
      "blocked       9.850510\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'The test data % failure rate by {(false_predictions.question_language.value_counts() / test_df.question_language.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_country_code.value_counts() / test_df.question_user_country_code.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_status.value_counts() / test_df.question_user_status.value_counts()) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3681182e-40b8-4790-8532-e3188a7d8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 60 failure rates \n",
      "       question_topic  failed_prediction  total  percentage_failed\n",
      "0           chickpea                 14     14         100.000000\n",
      "1          asparagus                 11     11         100.000000\n",
      "2           snap-pea                  7      7         100.000000\n",
      "3         gooseberry                 27     27         100.000000\n",
      "4            apricot                  6      6         100.000000\n",
      "5        castor-bean                  6      6         100.000000\n",
      "6              lupin                  5      5         100.000000\n",
      "7                rye                  5      5         100.000000\n",
      "8            setaria                  3      3         100.000000\n",
      "9           mulberry                  3      3         100.000000\n",
      "10        blackberry                  4      4         100.000000\n",
      "11             vetch                  4      4         100.000000\n",
      "12      purple-vetch                  1      1         100.000000\n",
      "13          leucaena                  3      3         100.000000\n",
      "14         cranberry                  1      1         100.000000\n",
      "15            celery                 10     10         100.000000\n",
      "16             chard                 17     17         100.000000\n",
      "17             peach                 92    106          86.792453\n",
      "18              flax                 43     52          82.692308\n",
      "19              pear                176    219          80.365297\n",
      "20  black-nightshade                 27     34          79.411765\n",
      "21         courgette                 28     38          73.684211\n",
      "22         safflower                 98    144          68.055556\n",
      "23        guinea-pig                 22     33          66.666667\n",
      "24       sudan-grass                 11     17          64.705882\n",
      "25            clover                 87    138          63.043478\n",
      "26             guava                173    275          62.909091\n",
      "27            radish                 46     75          61.333333\n",
      "28       cauliflower                 15     25          60.000000\n",
      "29              leek                  7     12          58.333333\n",
      "30    collard-greens                 60    105          57.142857\n",
      "31            cereal               1660   2956          56.156969\n",
      "32         caliandra                  5      9          55.555556\n",
      "33      napier-grass                910   1707          53.309900\n",
      "34      sweet-potato               1304   2629          49.600609\n",
      "35            acacia                 61    125          48.800000\n",
      "36             sisal                156    329          47.416413\n",
      "37            greens                127    270          47.037037\n",
      "38          amaranth                 58    124          46.774194\n",
      "39             olive                383    819          46.764347\n",
      "40             plant              11870  27895          42.552429\n",
      "41             wheat               2105   4960          42.439516\n",
      "42          plantain               1418   3386          41.878323\n",
      "43            chilli                603   1467          41.104294\n",
      "44        eucalyptus                166    462          35.930736\n",
      "45            lucern                 21     60          35.000000\n",
      "46       french-bean                110    323          34.055728\n",
      "47            sesame                 20     59          33.898305\n",
      "48            cyprus                 21     62          33.870968\n",
      "49          snow-pea                 65    198          32.828283\n",
      "50            barley                 21     66          31.818182\n",
      "51         aubergine                422   1388          30.403458\n",
      "52            pigeon                431   1462          29.480164\n",
      "53           parsley                 62    212          29.245283\n",
      "54            squash                 63    219          28.767123\n",
      "55     finger-millet                 42    150          28.000000\n",
      "56           tilapia                113    406          27.832512\n",
      "57             lemon                 72    259          27.799228\n",
      "58               cat                313   1167          26.820908\n",
      "59       boma-rhodes                 38    147          25.850340\n",
      "\n",
      "The bottom 60 failure rates \n",
      "     question_topic  failed_prediction   total  percentage_failed\n",
      "88      strawberry                 32     213          15.023474\n",
      "89            duck                652    4350          14.988506\n",
      "90           camel                 92     621          14.814815\n",
      "91            soya                202    1414          14.285714\n",
      "92           miraa                 40     282          14.184397\n",
      "93        rapeseed                 21     153          13.725490\n",
      "94             yam                 88     664          13.253012\n",
      "95          animal               1474   11157          13.211437\n",
      "96        capsicum                350    2741          12.769062\n",
      "97           grape                 14     110          12.727273\n",
      "98        broccoli                  4      32          12.500000\n",
      "99          millet                551    4723          11.666314\n",
      "100        poultry               6348   54449          11.658616\n",
      "101           taro                 66     574          11.498258\n",
      "102       beetroot                 45     404          11.138614\n",
      "103        spinach                194    1806          10.741971\n",
      "104           tree                664    6214          10.685549\n",
      "105        cabbage               1303   12372          10.531846\n",
      "106     corriander                  6      57          10.526316\n",
      "107        chicken               8865   86179          10.286729\n",
      "108           chia                 19     191           9.947644\n",
      "109        pumpkin                112    1219           9.187859\n",
      "110      vegetable                544    6019           9.038046\n",
      "111           kale                502    5626           8.922858\n",
      "112       cucumber                 17     198           8.585859\n",
      "113       mushroom                121    1418           8.533145\n",
      "114         carrot                315    3744           8.413462\n",
      "115        tobacco                 81    1014           7.988166\n",
      "116     cashew-nut                 91    1167           7.797772\n",
      "117            pig               1898   24760           7.665590\n",
      "118     sugar-cane                381    4977           7.655214\n",
      "119      livestock                529    6998           7.559303\n",
      "120  passion-fruit                561    7432           7.548439\n",
      "121           crop               1382   18333           7.538319\n",
      "122     pigeon-pea                 35     465           7.526882\n",
      "123           goat               1789   23893           7.487549\n",
      "124          onion               1171   16277           7.194200\n",
      "125          cocoa                 43     598           7.190635\n",
      "126          sheep                572    8172           6.999511\n",
      "127        avocado                181    2836           6.382228\n",
      "128      sunflower                205    3227           6.352650\n",
      "129            dog                196    3109           6.304278\n",
      "130         coffee               1069   17425           6.134864\n",
      "131           fish                293    4916           5.960130\n",
      "132        cassava                324    5464           5.929722\n",
      "133         potato               1192   20303           5.871054\n",
      "134          apple                 55     943           5.832450\n",
      "135         ginger                 85    1461           5.817933\n",
      "136            bee                332    5814           5.710354\n",
      "137          maize               5768  107113           5.384967\n",
      "138      pineapple                 74    1414           5.233380\n",
      "139         peanut                379    7377           5.137590\n",
      "140           bean               1774   36889           4.809022\n",
      "141         cotton                583   12296           4.741379\n",
      "142         banana                687   14713           4.669340\n",
      "143            tea                211    4572           4.615048\n",
      "144         cattle               3622   85005           4.260926\n",
      "145         tomato               2697   65202           4.136376\n",
      "146           rice                609   15020           4.054594\n",
      "147         rabbit                443   16887           2.623320\n"
     ]
    }
   ],
   "source": [
    "question_topic_failed = false_predictions.question_topic.value_counts().rename_axis('question_topic').reset_index(name='failed_prediction')\n",
    "question_topic_total = test_df.question_topic_x.value_counts().rename_axis('question_topic').reset_index(name='total')\n",
    "question_topic = pd.merge(question_topic_failed, question_topic_total, how='inner')\n",
    "question_topic['percentage_failed'] = (question_topic['failed_prediction'] / question_topic['total']) * 100\n",
    "question_topic = question_topic.sort_values(by=['percentage_failed'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f'The top 60 failure rates \\n {question_topic.head(60)}\\n')\n",
    "print(f'The bottom 60 failure rates \\n {question_topic.tail(60)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a48a3-5e29-4bc9-ae29-c85c19c49387",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec2735ac-115c-478a-b46d-6c770a11f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel sentences_topic_exist\n",
    "# %xdel X \n",
    "# %xdel X_train_df\n",
    "# %xdel X_test_df\n",
    "# %xdel y_train_df\n",
    "# %xdel y_test_df\n",
    "# %xdel X_train\n",
    "# %xdel X_test\n",
    "# %xdel y_train_one_hot\n",
    "# %xdel y_train\n",
    "# %xdel y_test_one_hot\n",
    "# %xdel y_test\n",
    "# %xdel y_pred\n",
    "# %xdel false_predictions\n",
    "# %xdel df_topic_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a5ba5-a931-45b1-bbd4-096fc8d4c008",
   "metadata": {},
   "source": [
    "## Make Predictions For Missing question_topic Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2052611-2997-4b2a-b43e-da411a9cf572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m     2/110555\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28:44\u001b[0m 81ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 21:27:43.624935: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2787730452 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110555/110555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 21:34:07.960950: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2094335568 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Create X input and make predictions\n",
    "X_topic_null_predict = np.array(sequence.pad_sequences(X_topic_null, maxlen=max_len))\n",
    "y_pred_topic_null = model.predict(X_topic_null_predict)\n",
    "\n",
    "# Convert predictions to labels\n",
    "topic_null_predictions = [one_hot_columns[i] for i in np.argmax(y_pred_topic_null, axis=1)]\n",
    "\n",
    "# Insert predictions into 'question_topic' column for null dataframe\n",
    "df_topic_null['question_topic'] = topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0555e7d-66bb-4a23-9162-e167b3c35d03",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f653521-2b59-4fec-b162-158963a4a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel X_topic_null_predict\n",
    "# %xdel y_pred_topic_null\n",
    "# %xdel topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b97247-398a-40fe-856e-603baa6ce01f",
   "metadata": {},
   "source": [
    "# Export To CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf1aa7d-d629-4264-b110-54dee3e903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full datset without missing values for 'question_topic'\n",
    "# NB: Could not do this before due to resource limit on my computer\n",
    "chunks = pd.read_csv('../data/question_topic_valid.csv',\n",
    "                     dtype={'question_user_gender': str, 'response_user_gender': str}, # Removes mixed dtypes error message\n",
    "                     chunksize=100000\n",
    "                    )\n",
    "df_topic_exists = pd.DataFrame()\n",
    "\n",
    "for chunk in chunks:\n",
    "    df_topic_exists = pd.concat([df_topic_exists,chunk], axis=0)\n",
    "\n",
    "\n",
    "# Combine dataset without missing values with the predicted values to recreate the full dataset\n",
    "df_no_missing = pd.concat([df_topic_exists, df_topic_null], axis=0)\n",
    "\n",
    "\n",
    "# Export the predicted values only and the full dataset now with no missing values\n",
    "df_topic_null.to_csv('../data/question_topic_predicted.csv', index=False)\n",
    "df_no_missing.to_csv('../data/question_topic_no_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302ce69-b843-42f8-9a56-be94dd2d81a3",
   "metadata": {},
   "source": [
    "## Suggestions\n",
    "\n",
    "Amongst the values within 'question_topic' are some that are more generic than others. These include 'plant', 'animal', 'poultry', 'bird', 'grass', 'crop', 'vegetable', and 'tree'.\n",
    "\n",
    "'plant' from 'question_topic' had a very poor accuracy rate despite the high number of records in the train and test dataset. In at least some cases, another more specific label, as used for other records, might have been a more accurate and descriptive topic. Making such changes could potentially improve the accuracy of the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b881cf-c05a-45a0-9faa-f70c4aef8730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
