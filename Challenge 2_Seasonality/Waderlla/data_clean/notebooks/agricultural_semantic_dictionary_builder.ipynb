{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New word extraction and category dictionary pipeline\n",
        "\n",
        "This document describes the full pipeline used to build a multilingual agricultural keyword dictionary from a large corpus of farmer questions.\n",
        "\n",
        "The process consists of three main steps:\n",
        "\n",
        "1. **New word extraction** from the raw question corpus.  \n",
        "2. **External classification** of terms into agricultural categories using a GPT-based model.  \n",
        "3. **Merging JSON outputs** into a single consolidated category dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1 – New word extraction from the question corpus\n",
        "\n",
        "The goal of the first step is to scan a very large CSV file with farmer questions, extract word-like tokens, and collect only those words that have **not** been seen in previous runs.\n",
        "\n",
        "The script in this notebook:\n",
        "\n",
        "1. **Loads previously seen words**  \n",
        "   - Reads the cumulative file `words_seen.csv` (if it exists).  \n",
        "   - The file is expected to contain a single column `word`.  \n",
        "   - The words are loaded into a Python `set`, optionally lower-cased.  \n",
        "   - This set is used to skip tokens that were already collected in earlier runs.\n",
        "\n",
        "2. **Streams the main CSV file in chunks**  \n",
        "   - Reads `raw_challenge_2_seasonality.csv` using only the `question_content` column.  \n",
        "   - The file is processed in chunks of `CHUNKSIZE` rows (e.g. 100 000) to avoid memory issues.  \n",
        "   - Rows without text in `question_content` are dropped.\n",
        "\n",
        "3. **Tokenises each question**  \n",
        "   - For each question, the text is optionally converted to lowercase (when `LOWERCASE = True`).  \n",
        "   - Tokens are extracted using a simple regex pattern `\\w+` (letters, digits, underscore).  \n",
        "   - Tokens shorter than `MIN_WORD_LENGTH` (e.g. `< 2` characters) are discarded.  \n",
        "   - Only tokens **not present** in `words_seen.csv` are counted.\n",
        "\n",
        "4. **Counts new words**  \n",
        "   - A `Counter` object accumulates occurrences of each *new* token across all chunks.  \n",
        "   - After the entire file is processed, the script reports how many distinct new words were found.\n",
        "\n",
        "5. **Filters rare words**  \n",
        "   - The counter is converted to a DataFrame with columns `word` and `count`.  \n",
        "   - Words that occur fewer than `MIN_COUNT` times (e.g. 1) are removed.  \n",
        "   - If no words remain after filtering, the script exits.\n",
        "\n",
        "6. **Updates the cumulative word list**  \n",
        "   - From the filtered DataFrame, only the `word` column is taken.  \n",
        "   - If `words_seen.csv` already exists, the new words are appended (no header).  \n",
        "   - If it does not exist, a new file is created with a header.  \n",
        "   - As a result, each word appears at most once in `words_seen.csv`.\n",
        "\n",
        "7. **Creates batch files for downstream processing**  \n",
        "   - The filtered `(word, count)` table is split into batches of size `BATCH_SIZE` (e.g. 3000).  \n",
        "   - Each batch is saved as a separate CSV file named:  \n",
        "     `session_batch_001.csv`, `session_batch_002.csv`, etc.  \n",
        "   - These batch files contain the most frequent **new** words and are used as input for the classification step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "\n",
        "INPUT_CSV = \"raw_challenge_2_seasonality.csv\"\n",
        "TEXT_COL = \"question_content\"\n",
        "\n",
        "# Cumulative file with all words ever seen (append, single column 'word')\n",
        "CUMULATIVE_OUTPUT = \"words_seen.csv\"\n",
        "\n",
        "# Output batches, e.g. session_batch_001.csv, 002, ...\n",
        "BATCH_PREFIX = \"session_batch_\"\n",
        "BATCH_SIZE = 3000\n",
        "\n",
        "# Number of rows per chunk when reading the large CSV\n",
        "CHUNKSIZE = 100_000\n",
        "\n",
        "# Word filters\n",
        "MIN_WORD_LENGTH = 2   # ignore tokens shorter than this\n",
        "MIN_COUNT = 2         # drop words that occur only once\n",
        "\n",
        "LOWERCASE = True\n",
        "\n",
        "# Simple pattern for a \"word\"\n",
        "TOKEN_PATTERN = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "def load_already_seen_words(path: str) -> set:\n",
        "    \"\"\"Load words previously collected in earlier runs.\n",
        "\n",
        "    Expects a CSV file with a 'word' column.\n",
        "    Returns an empty set if the file does not exist.\n",
        "    \"\"\"\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        return set()\n",
        "\n",
        "    df = pd.read_csv(p, usecols=[\"word\"], encoding=\"utf-8\")\n",
        "    words = df[\"word\"].astype(str)\n",
        "    if LOWERCASE:\n",
        "        words = words.str.lower()\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def tokenize(text: str):\n",
        "    \"\"\"Tokenise text into simple word-like tokens.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    if LOWERCASE:\n",
        "        text = text.lower()\n",
        "\n",
        "    tokens = TOKEN_PATTERN.findall(text)\n",
        "    return [t for t in tokens if len(t) >= MIN_WORD_LENGTH]\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1. Load words from previous runs to skip them\n",
        "    already_seen = load_already_seen_words(CUMULATIVE_OUTPUT)\n",
        "    print(f\"Loaded {len(already_seen)} words from cumulative file '{CUMULATIVE_OUTPUT}'.\")\n",
        "\n",
        "    counter = Counter()\n",
        "    total_rows = 0\n",
        "\n",
        "    # 2. Stream the large CSV file in chunks\n",
        "    for chunk in pd.read_csv(\n",
        "        INPUT_CSV,\n",
        "        sep=\";\",\n",
        "        usecols=[TEXT_COL],\n",
        "        chunksize=CHUNKSIZE,\n",
        "        encoding=\"utf-8\"\n",
        "    ):\n",
        "        total_rows += len(chunk)\n",
        "\n",
        "        # Drop rows without text\n",
        "        chunk = chunk.dropna(subset=[TEXT_COL])\n",
        "\n",
        "        for text in chunk[TEXT_COL]:\n",
        "            tokens = tokenize(text)\n",
        "            for tok in tokens:\n",
        "                if tok in already_seen:\n",
        "                    continue\n",
        "                counter[tok] += 1\n",
        "\n",
        "        print(f\"Processed {total_rows} rows...\")\n",
        "\n",
        "    print(f\"Number of NEW words (before count filter): {len(counter)}\")\n",
        "\n",
        "    if not counter:\n",
        "        print(\"No new words found – everything is already in the cumulative file.\")\n",
        "        return\n",
        "\n",
        "    # 3. All new words sorted by frequency\n",
        "    most_common = counter.most_common()\n",
        "    df_all = pd.DataFrame(most_common, columns=[\"word\", \"count\"])\n",
        "\n",
        "    # 4. Filter out words that are too rare\n",
        "    df_all = df_all[df_all[\"count\"] >= MIN_COUNT].reset_index(drop=True)\n",
        "    print(f\"After dropping words with count < {MIN_COUNT}, {len(df_all)} words remain.\")\n",
        "\n",
        "    if df_all.empty:\n",
        "        print(\"No words left to save after applying MIN_COUNT.\")\n",
        "        return\n",
        "\n",
        "    # 5. Append words to the cumulative file (only the 'word' column)\n",
        "    df_cum = df_all[[\"word\"]].copy()\n",
        "\n",
        "    cum_path = Path(CUMULATIVE_OUTPUT)\n",
        "    if cum_path.exists():\n",
        "        df_cum.to_csv(cum_path, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
        "        print(f\"Appended {len(df_cum)} words to cumulative file '{CUMULATIVE_OUTPUT}'.\")\n",
        "    else:\n",
        "        df_cum.to_csv(cum_path, index=False, encoding=\"utf-8\")\n",
        "        print(f\"Created new cumulative file '{CUMULATIVE_OUTPUT}' with {len(df_cum)} words.\")\n",
        "\n",
        "    # 6. Generate word batches of size BATCH_SIZE\n",
        "    num_words = len(df_all)\n",
        "    batch_count = 0\n",
        "\n",
        "    for start in range(0, num_words, BATCH_SIZE):\n",
        "        batch = df_all.iloc[start:start + BATCH_SIZE]\n",
        "        batch_count += 1\n",
        "        batch_filename = f\"{BATCH_PREFIX}{batch_count:03d}.csv\"\n",
        "        batch.to_csv(batch_filename, index=False, encoding=\"utf-8\")\n",
        "        print(f\"Saved batch {batch_count} ({len(batch)} words) to '{batch_filename}'.\")\n",
        "\n",
        "    print(f\"Done. Created {batch_count} batch files with up to {BATCH_SIZE} words each.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2 – External classification of terms into categories (LLM step)\n",
        "\n",
        "The batch files from Step 1 (e.g. `session_batch_001.csv`, `session_batch_002.csv`, …) contain lists of high-frequency terms extracted from the corpus.  \n",
        "These terms are then assigned to agricultural categories by a GPT-based conversational model configured specifically for this task.\n",
        "\n",
        "Because of input length limits, each ~3000-word batch is processed in smaller chunks of about **250 terms**:\n",
        "\n",
        "- From each `session_batch_XXX.csv`, a sublist of roughly 250 terms is taken.  \n",
        "- This list is sent to the GPT-based model, which uses a fixed prompt describing:  \n",
        "  - the category definitions,  \n",
        "  - and the required output format (a single JSON object).  \n",
        "- The model returns **one JSON object** mapping categories to lists of terms.\n",
        "\n",
        "The JSON returned by the model obeys these rules:\n",
        "\n",
        "- Keys are fixed category names:  \n",
        "  - `planting_growing`  \n",
        "  - `livestock`  \n",
        "  - `pests_disease`  \n",
        "  - `timing_harvest`  \n",
        "  - `weather`  \n",
        "  - `market_price`  \n",
        "- Values are lists of **original terms** assigned to each category (no translation or rewriting).  \n",
        "- Terms classified as `other` are omitted and do **not** appear in the JSON.  \n",
        "- Pure numbers and numeric suffixes are ignored.\n",
        "\n",
        "**Privacy note:**  \n",
        "The model never receives full question texts or any user-level context. Only isolated words or very short expressions extracted from the corpus are sent. This means no original questions or sentences are transferred — only a de-contextualised vocabulary of terms.\n",
        "\n",
        "**Language-independence:**  \n",
        "Because only tokens are sent (without assuming any specific language), the resulting dictionary is language-agnostic. The model assigns terms to categories based on their agricultural meaning, regardless of whether they are in English, Swahili or local languages.\n",
        "\n",
        "Each JSON response from the model is saved as a separate file, for example:\n",
        "\n",
        "```text\n",
        "words_categories_batch_001.json\n",
        "words_categories_batch_002.json\n",
        "words_categories_batch_003.json\n",
        "…\n",
        "```\n",
        "\n",
        "Together, these files form a set of partial dictionaries with term–category assignments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3 – Merging JSON files into a single category dictionary\n",
        "\n",
        "The final Python step merges the JSON files produced by the classifier into one consolidated dictionary.\n",
        "\n",
        "Although the pipeline is capable of processing all files matching  \n",
        "`words_categories_batch_*.json`, **only the first three batches were intentionally used**:\n",
        "\n",
        "```text\n",
        "words_categories_batch_001.json\n",
        "words_categories_batch_002.json\n",
        "words_categories_batch_003.json\n",
        "```\n",
        "\n",
        "The remaining batches contained words with lower global frequency. These terms were often ambiguous, strongly dialect-specific or appeared only a few times in the corpus, which introduced noise into the final dictionary. To maintain quality and avoid adding unstable or unreliable entries, only the top three files (covering the most frequent and semantically stable terms) were merged.\n",
        "\n",
        "Each selected JSON file has the form:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"planting_growing\": [\"term1\", \"term2\", \"...\"],\n",
        "  \"livestock\": [\"term3\", \"...\"],\n",
        "  \"pests_disease\": [\"...\"],\n",
        "  \"timing_harvest\": [\"...\"],\n",
        "  \"weather\": [\"...\"],\n",
        "  \"market_price\": [\"...\"]\n",
        "}\n",
        "```\n",
        "\n",
        "The merging logic is implemented in the code cell below. It:\n",
        "\n",
        "1. Loads the selected JSON files.  \n",
        "2. For each category, collects all terms using a Python `set` to ensure uniqueness.  \n",
        "3. Converts each set into a sorted list.  \n",
        "4. Writes the final consolidated dictionary to:\n",
        "\n",
        "```text\n",
        "words_categories_merged.json\n",
        "```\n",
        "\n",
        "The output file contains a clean, deduplicated dictionary of high-frequency agricultural terms, aggregated across the three most reliable classification batches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "# Directory with words_categories_batch_XXX.json files\n",
        "INPUT_DIR = Path(\".\")  # current directory; change if needed\n",
        "\n",
        "# File name pattern\n",
        "FILE_PATTERN = \"words_categories_batch_*.json\"\n",
        "\n",
        "# Output file\n",
        "OUTPUT_FILE = Path(\"words_categories_merged.json\")\n",
        "\n",
        "\n",
        "def merge_word_category_batches(input_dir: Path, pattern: str) -> dict:\n",
        "    \"\"\"Merge all JSON files matching the pattern into a single dictionary.\n",
        "\n",
        "    Each input file is expected to contain a JSON object of the form:\n",
        "        category -> list of words\n",
        "\n",
        "    The result is a dictionary:\n",
        "        category -> sorted list of unique words.\n",
        "    \"\"\"\n",
        "    merged = {}  # category -> set(words)\n",
        "\n",
        "    json_files = sorted(input_dir.glob(pattern))\n",
        "    if not json_files:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No files matching pattern: {pattern} found in {input_dir}\"\n",
        "        )\n",
        "\n",
        "    print(f\"Found {len(json_files)} files to merge.\")\n",
        "\n",
        "    for json_path in json_files:\n",
        "        print(f\"Processing: {json_path}\")\n",
        "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if not isinstance(data, dict):\n",
        "            print(f\"  [WARNING] {json_path} does not contain a dict – skipping.\")\n",
        "            continue\n",
        "\n",
        "        for category, words in data.items():\n",
        "            if not isinstance(words, list):\n",
        "                print(\n",
        "                    f\"  [WARNING] {json_path} -> key '{category}' is not a list – skipping this key.\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            if category not in merged:\n",
        "                merged[category] = set()\n",
        "\n",
        "            merged[category].update(words)\n",
        "\n",
        "    # convert sets to sorted lists\n",
        "    merged_as_lists = {cat: sorted(list(words)) for cat, words in merged.items()}\n",
        "    return merged_as_lists\n",
        "\n",
        "\n",
        "def main():\n",
        "    merged = merge_word_category_batches(INPUT_DIR, FILE_PATTERN)\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(merged, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved merged dictionary to: {OUTPUT_FILE}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d237f85",
      "metadata": {},
      "source": [
        "## Step 4 – Assigning a single category to every question in the corpus\n",
        "\n",
        "In the previous steps:\n",
        "\n",
        "1. High-frequency words were extracted from the large question corpus (`raw_challenge_2_seasonality.csv`).\n",
        "2. A GPT-based model was used to assign these words to agricultural categories and save the results as JSON batches.\n",
        "3. Selected JSON files were merged into a consolidated dictionary `words_categories_merged.json`, where each category maps to a list of terms.\n",
        "\n",
        "The goal of this final step is to **assign exactly one category to every question** in the original CSV file, using that consolidated dictionary.\n",
        "\n",
        "### What this step does\n",
        "\n",
        "1. **Loads the consolidated dictionary**\n",
        "\n",
        "   The file `words_categories_merged.json` is read into memory as a dictionary of the form:\n",
        "\n",
        "   `category -> list of words`.\n",
        "\n",
        "2. **Normalises all words in the dictionary**\n",
        "\n",
        "   Each word is:\n",
        "\n",
        "   - converted to string,\n",
        "   - lowercased,\n",
        "   - stripped of surrounding whitespace,\n",
        "   - optionally cleaned from a leading `@`.\n",
        "\n",
        "   For every category, a `set` of normalised words is built. Using sets makes checking membership and intersections efficient.\n",
        "\n",
        "3. **Defines category priority and ensures `other` is last**\n",
        "\n",
        "   The category list is taken from the dictionary keys, preserving the original JSON order (which acts as a priority order).  \n",
        "   The special category `other` is added if missing and always moved to the **end** of the list. This guarantees that:\n",
        "\n",
        "   - all “regular” categories are checked first,\n",
        "   - `other` is only used as a fallback if nothing else matches.\n",
        "\n",
        "4. **Defines the classification function for a single text**\n",
        "\n",
        "   The function `get_category_for_text(text)`:\n",
        "\n",
        "   - returns `\"other\"` if the input is not a string,\n",
        "   - tokenises the text into `\\w+` tokens (Unicode, lowercased),\n",
        "   - builds a set of tokens found in the text,\n",
        "   - iterates over categories in the defined priority order (excluding `other`),\n",
        "   - for each category, checks whether there is **any intersection** between:\n",
        "     - the set of words associated with the category, and\n",
        "     - the set of tokens appearing in the text,\n",
        "   - returns the first category that has at least one matching word,\n",
        "   - returns `\"other\"` if no category matches.\n",
        "\n",
        "   This guarantees **exactly one category per question**, based on the first category that matches any token in the text.\n",
        "\n",
        "5. **Streams the large CSV file and writes the result**\n",
        "\n",
        "   The input file:\n",
        "\n",
        "   - `CSV_INPUT = \"raw_challenge_2_seasonality.csv`  \n",
        "   - `TEXT_COL = \"question_content\"`\n",
        "\n",
        "   is processed in chunks (`CHUNKSIZE` rows at a time), using `sep=\";\"` and UTF-8 encoding.\n",
        "\n",
        "   For each chunk:\n",
        "\n",
        "   - a new column `category` is created by applying `get_category_for_text` to the `question_content` column,\n",
        "   - only `question_content` and `category` are kept,\n",
        "   - rows are appended to the output CSV file:\n",
        "\n",
        "   - `CSV_OUTPUT = \"topics_with_categories_one_col.csv\"`\n",
        "\n",
        "   The output file is created once at the beginning (removing any previous version). A header is written only for the first chunk; subsequent chunks are appended without a header.\n",
        "\n",
        "At the end of this step, the file `topics_with_categories_one_col.csv` contains:\n",
        "\n",
        "- `question_content` – the original question text,\n",
        "- `category` – the assigned category (e.g. `livestock`, `market_price`, `pests_disease`, …, or `other`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68da150d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# ===== CONFIGURATION =====\n",
        "CSV_INPUT = \"raw_challenge_2_seasonality.csv\"\n",
        "DICT_JSON = \"words_categories_merged.json\"\n",
        "CSV_OUTPUT = \"topics_with_categories_one_col.csv\"\n",
        "\n",
        "TEXT_COL = \"question_content\"\n",
        "CHUNKSIZE = 100_000\n",
        "\n",
        "# ===== 1. Load dictionary: category -> [words] =====\n",
        "with open(DICT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    # Python 3.7+ preserves key order = order from JSON\n",
        "    cat_to_words_raw = json.load(f)\n",
        "\n",
        "\n",
        "def norm_word(w):\n",
        "    \"\"\"Normalize a single word from the dictionary.\"\"\"\n",
        "    w = str(w).lower().strip()\n",
        "    if w.startswith(\"@\"):\n",
        "        w = w[1:]\n",
        "    return w\n",
        "\n",
        "\n",
        "# Normalized dictionary: category -> set(words)\n",
        "normalized_cat_to_words = {}\n",
        "for cat, words in cat_to_words_raw.items():\n",
        "    if not isinstance(words, (list, tuple)):\n",
        "        # if value is not a list/tuple, skip\n",
        "        continue\n",
        "    norm_set = set()\n",
        "    for w in words:\n",
        "        nw = norm_word(w)\n",
        "        if nw:\n",
        "            norm_set.add(nw)\n",
        "    normalized_cat_to_words[cat] = norm_set\n",
        "\n",
        "# Category order as in JSON\n",
        "categories_in_order = list(normalized_cat_to_words.keys())\n",
        "\n",
        "# Ensure \"other\" exists and is at the END\n",
        "if \"other\" not in categories_in_order:\n",
        "    categories_in_order.append(\"other\")\n",
        "else:\n",
        "    # move \"other\" to the end\n",
        "    categories_in_order = [c for c in categories_in_order if c != \"other\"] + [\"other\"]\n",
        "\n",
        "print(\"Category order (priority):\", categories_in_order)\n",
        "\n",
        "# ===== 2. Function: one category per text =====\n",
        "token_re = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "def get_category_for_text(text) -> str:\n",
        "    \"\"\"Return exactly one category for the given text.\"\"\"\n",
        "    # missing / wrong type -> other\n",
        "    if not isinstance(text, str):\n",
        "        return \"other\"\n",
        "\n",
        "    # set of tokens from text (lowercase)\n",
        "    tokens = {t.lower() for t in token_re.findall(text)}\n",
        "\n",
        "    # iterate over categories in priority order\n",
        "    for cat in categories_in_order:\n",
        "        if cat == \"other\":\n",
        "            # handle \"other\" at the very end if nothing matches\n",
        "            continue\n",
        "\n",
        "        cat_words = normalized_cat_to_words.get(cat, set())\n",
        "        # if there is any intersection -> choose this category\n",
        "        if cat_words & tokens:\n",
        "            return cat\n",
        "\n",
        "    # nothing matched -> \"other\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "# ===== 3. Process the CSV in chunks =====\n",
        "out_path = Path(CSV_OUTPUT)\n",
        "if out_path.exists():\n",
        "    out_path.unlink()\n",
        "\n",
        "header_written = False\n",
        "\n",
        "for chunk in pd.read_csv(\n",
        "    CSV_INPUT,\n",
        "    chunksize=CHUNKSIZE,\n",
        "    encoding=\"utf-8\",\n",
        "    on_bad_lines=\"skip\",\n",
        "    sep=\";\"      # adjust if your CSV uses a different separator\n",
        "):\n",
        "    if TEXT_COL not in chunk.columns:\n",
        "        raise ValueError(f\"Column '{TEXT_COL}' not found in input file!\")\n",
        "\n",
        "    # one category per row\n",
        "    chunk[\"category\"] = chunk[TEXT_COL].apply(get_category_for_text)\n",
        "\n",
        "    cols_to_save = [TEXT_COL, \"category\"]\n",
        "\n",
        "    chunk[cols_to_save].to_csv(\n",
        "        out_path,\n",
        "        mode=\"a\",\n",
        "        index=False,\n",
        "        header=not header_written,\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    header_written = True\n",
        "\n",
        "print(f\"Result saved to: {CSV_OUTPUT}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1616386d",
      "metadata": {},
      "source": [
        "### Classification quality\n",
        "\n",
        "To evaluate the quality of dictionary-based categorisation, a manual review of 200 randomly selected questions was performed, covering the major languages present in the dataset (e.g., Swahili, Luganda, Runyankole, English).\n",
        "Each question was verified against its assigned category and corrected where necessary.\n",
        "\n",
        "Based on this sample, the estimated overall accuracy of the category assignment is approximately 85%.\n",
        "\n",
        "This value should be interpreted as an approximation of real-world performance, as some ambiguity remains in multi-topic questions and very short or context-poor inputs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
